{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9996344,"sourceType":"datasetVersion","datasetId":6152552},{"sourceId":9998440,"sourceType":"datasetVersion","datasetId":6154021},{"sourceId":9998609,"sourceType":"datasetVersion","datasetId":6154146},{"sourceId":9998909,"sourceType":"datasetVersion","datasetId":6154354}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:10:26.229494Z","iopub.execute_input":"2024-11-24T15:10:26.230097Z","iopub.status.idle":"2024-11-24T15:10:26.553479Z","shell.execute_reply.started":"2024-11-24T15:10:26.230062Z","shell.execute_reply":"2024-11-24T15:10:26.552678Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/pegasus/Screenshot 2024-11-24 161819.png\n/kaggle/input/cifake-test/1018 (8).jpg\n/kaggle/input/cifake-test/1017 (6).jpg\n/kaggle/input/cifake-test/1018 (4).jpg\n/kaggle/input/cifake-test/1017 (7).jpg\n/kaggle/input/cifake-test/1018.jpg\n/kaggle/input/cifake-test/1019 (2).jpg\n/kaggle/input/cifake-test/1018 (10).jpg\n/kaggle/input/cifake-test/1019 (4).jpg\n/kaggle/input/cifake-test/1017 (9).jpg\n/kaggle/input/cifake-test/1017 (4).jpg\n/kaggle/input/cifake-test/1018 (9).jpg\n/kaggle/input/cifake-test/1018 (2).jpg\n/kaggle/input/cifake-test/1019 (5).jpg\n/kaggle/input/artifact-interiit/Artifact_Description.csv\n/kaggle/input/random-mirror-lady-fake-lol/Screenshot 2024-11-24 154214.png\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **Installing dependencies and setting up**","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/\n!git clone -b v1.0 https://github.com/camenduru/LLaVA\n%cd /kaggle/working/LLaVA\n\n!pip install -q transformers==4.36.2\n!pip install -q gradio .\n\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom llava.model import LlavaLlamaForCausalLM\nimport torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:10:33.765054Z","iopub.execute_input":"2024-11-24T15:10:33.765644Z","iopub.status.idle":"2024-11-24T15:13:56.778239Z","shell.execute_reply.started":"2024-11-24T15:10:33.765611Z","shell.execute_reply":"2024-11-24T15:13:56.777544Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'LLaVA'...\nremote: Enumerating objects: 1960, done.\u001b[K\nremote: Total 1960 (delta 0), reused 0 (delta 0), pack-reused 1960 (from 1)\u001b[K\nReceiving objects: 100% (1960/1960), 13.60 MiB | 42.99 MiB/s, done.\nResolving deltas: 100% (1173/1173), done.\n/kaggle/working/LLaVA\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nalbumentations 1.4.17 requires pydantic>=2.7.0, but you have pydantic 1.10.19 which is incompatible.\njupyterlab 4.2.5 requires httpx>=0.25.0, but you have httpx 0.24.0 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkaggle-environments 1.14.15 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\npytorch-lightning 2.4.0 requires torch>=2.1.0, but you have torch 2.0.1 which is incompatible.\nydata-profiling 4.10.0 requires pydantic>=2, but you have pydantic 1.10.19 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m[2024-11-24 15:13:44,620] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# **Using CLIP to get the top k artifacts using cosine similarities**","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\n\n# Load CSV data\ncsv_path = \"/kaggle/input/artifact-interiit/Artifact_Description.csv\"\ndata = pd.read_csv(csv_path, on_bad_lines=\"skip\")\n\n# Load CLIP model and processor\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Prepare artifact descriptions\nartifact_texts = data['Artifact'] + \": \" + data['Description']\ninputs_text = processor(text=artifact_texts.tolist(), return_tensors=\"pt\", padding=True)\n\n# Extract text features\nwith torch.no_grad():\n    text_features = model.get_text_features(**inputs_text)\n\n# Function to extract image features\ndef get_image_features(image_path):\n    # Load the image using Pillow\n    image = Image.open(image_path).convert(\"RGB\")\n    # Process the image\n    inputs_image = processor(images=image, return_tensors=\"pt\")\n    with torch.no_grad():\n        image_features = model.get_image_features(**inputs_image)\n    return image_features\n\n# Compare image features with artifact text features and save results\ndef find_artifacts(image_path, k=5):\n    image_features = get_image_features(image_path)\n    similarities = torch.nn.functional.cosine_similarity(image_features, text_features)\n    top_k_indices = similarities.topk(k).indices\n    top_k_similarities = similarities[top_k_indices].cpu().numpy()\n    results = data.iloc[top_k_indices].copy()\n    results['Cosine_Similarity'] = top_k_similarities\n    return results\n\n# Process all images in a folder and save results to a JSON file\ndef process_folder(image_folder, output_json, k=5):\n    results_dict = {}\n    \n    for image_name in os.listdir(image_folder):\n        image_path = os.path.join(image_folder, image_name)\n        \n        if os.path.isfile(image_path):  # Ensure it's a file\n            print(f\"Processing: {image_name}\")\n            \n            try:\n                top_artifacts = find_artifacts(image_path, k=k)\n                # Convert DataFrame to a dictionary format for JSON serialization\n                results_dict[image_name] = top_artifacts.to_dict(orient='records')\n            except Exception as e:\n                print(f\"Error processing {image_name}: {e}\")\n    \n    # Save the results dictionary to a JSON file\n    with open(output_json, 'w') as json_file:\n        json.dump(results_dict, json_file, indent=4)\n    print(f\"Results saved to {output_json}\")\n\n# Define the folder path and output JSON file\nimage_folder = \"/kaggle/input/cifake-test\"  # Replace with your folder path\noutput_json = \"/kaggle/working/top_artifacts.json\"\n\n# Run the folder processing\nprocess_folder(image_folder, output_json, k=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:14:14.451727Z","iopub.execute_input":"2024-11-24T16:14:14.452324Z","iopub.status.idle":"2024-11-24T16:14:17.113597Z","shell.execute_reply.started":"2024-11-24T16:14:14.452291Z","shell.execute_reply":"2024-11-24T16:14:17.112695Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Processing: 1018 (8).jpg\nProcessing: 1017 (6).jpg\nProcessing: 1018 (4).jpg\nProcessing: 1017 (7).jpg\nProcessing: 1018.jpg\nProcessing: 1019 (2).jpg\nProcessing: 1018 (10).jpg\nProcessing: 1019 (4).jpg\nProcessing: 1017 (9).jpg\nProcessing: 1017 (4).jpg\nProcessing: 1018 (9).jpg\nProcessing: 1018 (2).jpg\nProcessing: 1019 (5).jpg\nResults saved to /kaggle/working/top_artifacts.json\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# **Loading LLAVA into transformers**","metadata":{}},{"cell_type":"code","source":"\nmodel_path = \"4bit/llava-v1.5-13b-3GB\"\n    \nkwargs = {\"device_map\": \"auto\"}\nkwargs['load_in_4bit'] = True\nkwargs['quantization_config'] = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type='nf4'\n)\nmodel = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n\nvision_tower = model.get_vision_tower()\nif not vision_tower.is_loaded:\n    vision_tower.load_model()\nvision_tower.to(device='cuda')\nimage_processor = vision_tower.image_processor\n\nimport os\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.utils import disable_torch_init\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\nfrom transformers import TextStreamer\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:14:05.800597Z","iopub.execute_input":"2024-11-24T15:14:05.800861Z","iopub.status.idle":"2024-11-24T15:19:06.097842Z","shell.execute_reply.started":"2024-11-24T15:14:05.800834Z","shell.execute_reply":"2024-11-24T15:19:06.097058Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5304ce047d904ec09fceed3160403ff1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/33.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"039b515c929d473ca28a900399d94438"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8702c01887684bffa0110633908d372a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00009.bin:   0%|          | 0.00/2.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"153afda3e8824ebaa12ca5262cb5700d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00009.bin:   0%|          | 0.00/2.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45847ea451304685ad2b3ada21b5e68d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00009.bin:   0%|          | 0.00/2.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b0741e6f2634f4bb539a63b368b458d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00004-of-00009.bin:   0%|          | 0.00/2.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"645aa7f2bc054971ad08a13221f9d541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00005-of-00009.bin:   0%|          | 0.00/2.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fca23663e5c4393925de0d437748bc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00006-of-00009.bin:   0%|          | 0.00/2.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc5b0c5ebe34146b771f124cafc662f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00007-of-00009.bin:   0%|          | 0.00/2.87G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08e61f5768e9468386c40bb6f7f6371b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00008-of-00009.bin:   0%|          | 0.00/2.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d8d37ea805943448dc75cdfe476a6f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00009-of-00009.bin:   0%|          | 0.00/2.72G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96bfbb9e02694106be44fa4ad8765268"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa0991786e8045d19834a6c612bfeb74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f6e5096271047999dd4134f1fe38390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77b0d4202c04415884b063a299e4c055"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a74ae3c9bc74809850314e1ba7afc82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"035a84d00c844f48972353aa6136e5f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012af05717fd4a5fa90e42b3d9cb5642"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adb3e7650eea420f8bc715f8c0c731fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df924d629e74d488aa70884e78cc727"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def caption_image(image_file, prompt):\n    if image_file.startswith('http') or image_file.startswith('https'):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(image_file).convert('RGB')\n    disable_torch_init()\n    conv_mode = \"llava_v0\"\n    conv = conv_templates[conv_mode].copy()\n    roles = conv.roles\n    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().cuda()\n    inp = f\"{roles[0]}: {prompt}\"\n    inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n    conv.append_message(conv.roles[0], inp)\n    conv.append_message(conv.roles[1], None)\n    raw_prompt = conv.get_prompt()\n    input_ids = tokenizer_image_token(raw_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n    keywords = [stop_str]\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n    with torch.inference_mode():\n      output_ids = model.generate(input_ids, images=image_tensor, do_sample=True, temperature=0.2,\n                                  max_new_tokens=4096, use_cache=True, stopping_criteria=[stopping_criteria])\n    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n    conv.messages[-1][-1] = outputs\n    output = outputs.rsplit('</s>', 1)[0]\n    return image, output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:19:06.099596Z","iopub.execute_input":"2024-11-24T15:19:06.099872Z","iopub.status.idle":"2024-11-24T15:19:06.108206Z","shell.execute_reply.started":"2024-11-24T15:19:06.099844Z","shell.execute_reply":"2024-11-24T15:19:06.107328Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# **Final descriptions of all Artifacts**","metadata":{}},{"cell_type":"code","source":"json_file_path = \"/kaggle/working/top_artifacts.json\"  # Path to your JSON file\nwith open(json_file_path, 'r') as file:\n    ans = json.load(file)\nfor image_name, artifacts in ans.items():\n    print(f\"Image: {image_name}\")\n    for artifact in artifacts:\n        _, artifact['Img_desc'] = caption_image(f'/kaggle/input/cifake-test/{image_name}', f\"Explain why this image is AI generated because it has {artifact['Description']}. Also provide where in the image is this violation happening\")\n    print(f\"{image_name} processed successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:23:33.460170Z","iopub.execute_input":"2024-11-24T15:23:33.460572Z","iopub.status.idle":"2024-11-24T15:34:18.902759Z","shell.execute_reply.started":"2024-11-24T15:23:33.460539Z","shell.execute_reply":"2024-11-24T15:34:18.901894Z"}},"outputs":[{"name":"stdout","text":"Image: 1018 (8).jpg\n1018 (8).jpg processed successfully!\nImage: 1017 (6).jpg\n1017 (6).jpg processed successfully!\nImage: 1018 (4).jpg\n1018 (4).jpg processed successfully!\nImage: 1017 (7).jpg\n1017 (7).jpg processed successfully!\nImage: 1018.jpg\n1018.jpg processed successfully!\nImage: 1019 (2).jpg\n1019 (2).jpg processed successfully!\nImage: 1018 (10).jpg\n1018 (10).jpg processed successfully!\nImage: 1019 (4).jpg\n1019 (4).jpg processed successfully!\nImage: 1017 (9).jpg\n1017 (9).jpg processed successfully!\nImage: 1017 (4).jpg\n1017 (4).jpg processed successfully!\nImage: 1018 (9).jpg\n1018 (9).jpg processed successfully!\nImage: 1018 (2).jpg\n1018 (2).jpg processed successfully!\nImage: 1019 (5).jpg\n1019 (5).jpg processed successfully!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"output_json = \"/kaggle/working/top_artifacts_with_description.json\"\nwith open(output_json, 'w') as json_file:\n        json.dump(ans, json_file, indent=4)\nprint(\"JSON dumped successfully, task 2 pack\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T15:34:18.904130Z","iopub.execute_input":"2024-11-24T15:34:18.904386Z","iopub.status.idle":"2024-11-24T15:34:18.910798Z","shell.execute_reply.started":"2024-11-24T15:34:18.904361Z","shell.execute_reply":"2024-11-24T15:34:18.910039Z"}},"outputs":[{"name":"stdout","text":"JSON dumped successfully, task 2 pack\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}